from flask import Flask, render_template, request, jsonify
import google.generativeai as genai
import os
import time

app = Flask(__name__)

# API anahtarÄ±nÄ±zÄ± ortam deÄŸiÅŸkeni olarak ayarlayÄ±n
os.environ["GEMINI_API_KEY"] = "AIzaSyDP-HtHfjpz71rBnwJ8jwoT6gHqPRoFk1U"
genai.configure(api_key=os.environ["GEMINI_API_KEY"])

def upload_to_gemini(path, mime_type=None):
    """Uploads the given file to Gemini."""
    file = genai.upload_file(path, mime_type=mime_type)
    print(f"Uploaded file '{file.display_name}' as: {file.uri}")
    return file

def wait_for_files_active(files):
    """Waits for the given files to be active."""
    print("Waiting for file processing...")
    for name in (file.name for file in files):
        file = genai.get_file(name)
        while file.state.name == "PROCESSING":
            print(".", end="", flush=True)
            time.sleep(10)
            file = genai.get_file(name)
        if file.state.name != "ACTIVE":
            raise Exception(f"File {file.name} failed to process")
    print("...all files ready")
    print()

# DosyalarÄ± yÃ¼kleyin (dosya yollarÄ±nÄ± gÃ¼ncelleyin)
files = [
    upload_to_gemini(r"Takasbank.csv", mime_type="text/csv"),
]

# DosyalarÄ±n iÅŸlenmesini bekleyin
wait_for_files_active(files)

# Modeli oluÅŸturun
generation_config = {
    "temperature": 1,
    "top_p": 0.95,
    "top_k": 64,
    "max_output_tokens": 8192,
    "response_mime_type": "text/plain",
}

model = genai.GenerativeModel(
    model_name="gemini-1.5-flash",
    generation_config=generation_config,
)

# Chat oturumu baÅŸlatÄ±n
chat_session = model.start_chat(
    history=[
        {
            "role": "user",
            "parts": [
                files[0],
                "sen kiÅŸiye Ã¶zel bir yatÄ±rÄ±m danÄ±ÅŸmanÄ±sÄ±n sana yÃ¼klediÄŸimiz veriler doÄŸrultusunda yatÄ±rÄ±m ve fon ile ilgili sorulan sorularÄ± ceveplarsÄ±n bu sorular her ÅŸeyle alakalÄ± olabilir Ã¶nemli olan sadece sana yÃ¼klediÄŸimiz fonlar ile alakalÄ± olmasÄ± , yÃ¼klediÄŸimiz veriler ile alakasÄ±z sorularÄ± Bilmiyorum diyerek cevapla. Åimdi sana Ã¶rnek bir soru sÃ¶yleyeceÄŸim \"Bana TLC Fonunun son bir aylÄ±k getirisini sÃ¶yler misin?\" bu soruyu gÃ¶rdÃ¼ÄŸÃ¼nde sana yÃ¼klediÄŸimiz verilere giderek TLC Fonunu bulup ve istenilen cevabÄ± getirirsin. Ã–rneÄŸin ÅŸÃ¶yle bir soru gelirse \"BugÃ¼n hava durumu nedir?\" cevap olarak BilmediÄŸini yaratÄ±cÄ± bir dille sÃ¶yleyebilirsin",
            ],
        },
        {
            "role": "model",
            "parts": [
                "AnlaÅŸÄ±ldÄ±! Bana yÃ¼klediÄŸiniz fon verilerine gÃ¶re yatÄ±rÄ±m ve fonlarla ilgili sorularÄ± cevaplayabilirim. Veriler dÄ±ÅŸÄ±nda kalan sorular iÃ§in yaratÄ±cÄ± bir ÅŸekilde \"Bilmiyorum\" cevabÄ±nÄ± vereceÄŸim. \n\nÃ–rnek sorunuz \"Bana TLC Fonunun son bir aylÄ±k getirisini sÃ¶yler misin?\" iÃ§in, verilerimi kontrol edip TLC Fonu'nun son bir aylÄ±k getirisini size hemen bildiririm. \"BugÃ¼n hava durumu nedir?\" gibi alakasÄ±z bir soru iÃ§in ise yaratÄ±cÄ±lÄ±ÄŸÄ±mÄ± konuÅŸtururum. Belki \"ÃœzgÃ¼nÃ¼m, ben sadece finansal iklimlerde uzmanÄ±m, meteorolojik konularda bilgim yok.\" gibi bir cevap verebilirim.  \n\nSorularÄ±nÄ±zÄ± bekliyorum! ğŸ˜Š \n",
            ],
        },
    ]
)

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/send_message', methods=['POST'])
def send_message():
    user_message = request.json['message']
    response = chat_session.send_message(user_message)
    return jsonify({'response': response.text})

if __name__ == '__main__':
    app.run(debug=True)
